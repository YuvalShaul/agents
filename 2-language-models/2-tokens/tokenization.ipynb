{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "14b02b5d",
   "metadata": {},
   "source": [
    "## Tokenizing Lab\n",
    "\n",
    "These are several experiments with the [**tiktoken**](https://github.com/openai/tiktoken) library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6ee6a020",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tiktoken\n",
    "\n",
    "enc_legacy = tiktoken.get_encoding(\"cl100k_base\")     # cl100k_base = GPT-4 / GPT-3.5\n",
    "enc_modern = tiktoken.get_encoding(\"o200k_base\")      # o200k_base  = GPT-4o / GPT-4o-mini\n",
    "\n",
    "def run_tokenization(text, tokenizer):\n",
    "    tokens = tokenizer.encode(text)\n",
    "    print(tokens)\n",
    "    print( [tokenizer.decode_single_token_bytes(t).decode('utf-8', errors='replace') for t in tokens])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e51e5520",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[83, 1609, 5963, 374, 2294, 0]\n",
      "['t', 'ik', 'token', ' is', ' great', '!']\n",
      "[83, 8251, 2488, 382, 2212, 0]\n",
      "['t', 'ikt', 'oken', ' is', ' great', '!']\n"
     ]
    }
   ],
   "source": [
    "# 1. The Multi-word test\n",
    "\n",
    "text = \"tiktoken is great!\"\n",
    "run_tokenization(text, enc_legacy)\n",
    "run_tokenization(text, enc_modern)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ea895cd5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[15339]\n",
      "['hello']\n",
      "[24912]\n",
      "['hello']\n",
      "[9906]\n",
      "['Hello']\n",
      "[13225]\n",
      "['Hello']\n"
     ]
    }
   ],
   "source": [
    "# 2. The Case Sensitive test\n",
    "text = \"hello\"\n",
    "run_tokenization(text, enc_legacy)\n",
    "run_tokenization(text, enc_modern)\n",
    "\n",
    "text = \"Hello\"\n",
    "run_tokenization(text, enc_legacy)\n",
    "run_tokenization(text, enc_modern)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "aaec2fff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[59511, 50391, 37769, 251]\n",
      "['◊©', '◊ú', '◊ïÔøΩ', 'ÔøΩ']\n",
      "[106154]\n",
      "['◊©◊ú◊ï◊ù']\n"
     ]
    }
   ],
   "source": [
    "# 3. The Multilingual test (Huge difference here!)\n",
    "text = \"◊©◊ú◊ï◊ù\"\n",
    "run_tokenization(text, enc_legacy)\n",
    "run_tokenization(text, enc_modern)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27834b71",
   "metadata": {},
   "source": [
    "### Class Exercise\n",
    "- Write an example of an \"inefficient text\" (token terms):\n",
    "  - break sequence of spaces with low-value characters (instead of space-space-sapce-space interleave with dots or commas)\n",
    "  - Use rare unicode characters (for example  ùêáùêûùê•ùê•ùê® which is bold-unicode letters)\n",
    "  - Break Subword Associations - (e.g  U.S.A instead of USA)\n",
    "  - Space is attached to the next token, but newline isn't. Try words seperasted by a single space vs a new-line\n",
    "  - Write \"high-entropy\" text (like a password)\n",
    "- Now fix your example"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
